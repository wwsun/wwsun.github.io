---
created: 2026-01-08 17:10
url:
tags:
---
### 1. 核心定义：从“快思考”到“慢思考”

传统 LLM（如 GPT-4、Claude 3.5）通常采用的是 **“直觉式生成”**，就像人类的“快思考”，根据概率直接预测下一个词。而推理模型（如 OpenAI o1, DeepSeek-R1）引入了 **“思维链”（Chain of Thought, CoT）** 的强化。

- **System 1 (快思考)：** 反应快、基于直觉。传统模型直接给出答案。
    
- **System 2 (慢思考)：** 反应慢、有逻辑、会纠错。推理模型在给出答案前，会产生大量的内部推理 Token（Internal Reasoning Tokens）。
    

---

### 2. 它是如何实现的？（关键技术）

推理模型之所以聪明，并不是因为它背了更多书（预训练），而是因为它学会了“如何思考”（后训练）。

- **强化学习 (Reinforcement Learning, RL)：** 这是推理模型的灵魂。通过奖励机制，教模型在遇到难题时不要急着给答案，而是先去拆解问题。
    
- **推理缩放定律 (Inference-time Scaling Law)：** 这是一个颠覆性的发现：在模型推理时（即回答问题时）给它更多的计算资源（让它想得久一点），它的表现会持续提升。这不同于以往“堆参数、堆数据”的路径。
    
- **自我博弈与反思：** 模型在训练中会自己尝试不同的解题路径，如果发现某条路走不通（如代码运行报错或数学结果不匹配），它会学会回溯并尝试新方法。
    

---

### 3. 推理模型 vs 普通模型：有什么不同？

|**特性**|**普通指令微调模型 (Chat)**|**推理模型 (Reasoning)**|
|---|---|---|
|**生成速度**|几乎即时开始输出内容|会有明显的“Thinking”时间|
|**擅长领域**|创意写作、总结、日常闲聊|数学证明、复杂编程、逻辑谜题|
|**指令遵循**|对明确的 Prompt 响应极佳|对模糊的目标（Goal-oriented）更有韧性|
|**成本**|较低|较高（因为产生了很多不可见的推理 Token）|

---

### 4. 你应该知道的关键概念

- **思维链 (CoT):** 模型将复杂任务拆解为逻辑连贯的小步骤。
    
- **推理 Token (Reasoning Tokens):** 这是推理模型特有的，它们在后台运行，帮助模型思考，但最终答案中可能不直接显示。
    
- **过程奖励 (Process Reward vs. Outcome Reward):** 以前只看结果对不对，现在会对模型中间的每一步推导过程进行打分和奖励。
    
- **System Prompt 依赖度降低：** 推理模型不需要你写繁琐的“请一步步思考”，它天生就会这么做。
    
