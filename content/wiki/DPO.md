---
title: Direct Preference Optimization, 直接偏好优化
tags:
draft: false
description: DPO
url:
---
如果说传统的 RLHF (PPO) 是“绕远路”，那么 DPO 就是通过数学上的巧妙转换，直接“抄了近路”。

---

## 1. 核心直觉：模型即奖励模型

在 PPO 流程中，我们需要训练一个独立的 **Reward Model (RM)**，然后用强化学习去对齐。

DPO 的核心贡献在于：它在数学上证明了，**一个最优的语言模型策略（Policy）与它的奖励函数（Reward Function）之间存在闭式解（Closed-form solution）的对应关系。**

简单来说：**你不需要额外训练一个判官模型，你的大语言模型本身就可以通过概率分布差值，隐含地扮演判官的角色。**

---

## 2. DPO 的数学实现（CS 学生视角）

作为计算机系学生，你可以直接看它的损失函数（Loss Function）：

$$L_{DPO}(\pi_{\theta}; \pi_{ref}) = -E_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_{ref}(y_l|x)} \right) \right]$$

### 参数解析：

- $\pi_{\theta}$：你正在训练的模型（当前策略）。
    
- $\pi_{ref}$：参考模型（通常是刚做完 SFT 的那个基座）。
    
- $y_w$ (winning)：人类偏好的回答。
    
- $y_l$ (losing)：人类拒绝的回答。
    
- $\beta$：超参数，控制偏离参考模型的程度（类似 KL 惩罚系数）。
    

### 逻辑拆解：

公式括号里的内容是**两个 Log-Ratio 的差值**。

1. 第一个项计算当前模型相比参考模型，有多喜欢好答案 $y_w$。
    
2. 第二个项计算当前模型相比参考模型，有多喜欢坏答案 $y_l$。
    
3. DPO 的目标就是：**最大化这两者的差值**。即让模型“更喜欢好答案、更讨厌坏答案”。
    

---

## 3. DPO vs. PPO：工程侧的权衡

|**维度**|**RLHF (PPO)**|**DPO**|
|---|---|---|
|**模型数量**|4 个 (Actor, Critic, Ref, RM)|**2 个** (Policy, Ref)|
|**计算开销**|极大（需要在线采样，显存占用高）|**显著降低**（离线训练，类似 SFT）|
|**稳定性**|极差（对超参数极其敏感，容易崩）|**极高**（本质是分类任务，梯度稳定）|
|**调优难度**|专家级（需要平衡 RL 各项指标）|**开发友好**（类似微调，容易收敛）|
|**性能上限**|在复杂推理（Math/Code）中通常更强|在对话对齐中平起平坐，但逻辑稍逊|

---

## 4. 2026 年的新进展：DPO 的变体

DPO 虽然简洁，但也有缺点（比如容易对数据过拟合、在负样本上概率降得太低）。2025-2026 年，学术界提出了一些改进版：

- **TDPO (Token-level DPO)**：不仅看整句的偏好，还利用 KL 散度在 Token 级别进行精细化约束。
    
- **BPO (Balanced Preference Optimization)**：解决了 DPO 有时会导致好答案的绝对概率也下降的问题，通过平衡项确保模型在“选优”的同时不丢失语言表达能力。
    
- **DPO-RK / DPO-D (NeurIPS 2025)**：引入了“平局（Ties）”处理机制，针对模型输出平分秋色时的情况进行优化，提高了训练的数据利用率。
    

---

## 5. 为什么 DPO 统治了开源界？

目前的 Llama 3、Qwen 2 以及许多主流开源模型，在对齐阶段几乎都优先采用 DPO（或其变体 ORPO）。

原因很简单：**在有限的算力资源下，DPO 提供了最高的性价比。** 它可以直接在单机多卡上像做有监督微调一样完成对齐，而不需要维护一个复杂的分布式强化学习训练框架。
还是想看看如何在 PyTorch 中利用 `trl` 库几行代码实现 DPO 训练？**