---
title: RLHF 基于人类反馈的强化学习
tags:
draft: false
description: 基于人类反馈的强化学习
url:
---
可以将 **RLHF (Reinforcement Learning from Human Feedback，基于人类反馈的强化学习)** 理解为一个**将人类的主观偏好“数学化”并注入模型**的过程。

在预训练阶段，模型只是在学习“概率”，即如何根据上文预测下一个 Token。但模型并不懂得什么是“好话”、什么是“废话”、什么是“有毒的建议”。RLHF 的目的就是通过强化学习，让模型从“概率预测器”变成“好用的助手”。

典型的 RLHF 流程通常分为三个阶段：

---

## 1. 第一阶段：有监督微调 (SFT, Supervised Fine-Tuning)

**目标：给模型找个“样板间”。**

算法工程师会搜集一批高质量的“问题-答案”对（Prompt-Response），这些答案通常是由人类专家编写的。

- **做法**：在预训练模型的基础上，用这些数据进行有监督学习。
    
- **本质**：这是一个 **Imitation Learning（模仿学习）**。模型学会了人类回答问题的基本格式和风格。
    

---

## 2. 第二阶段：训练奖励模型 (RM, Reward Model)

**目标：训练一个“电子判官”，把人类的喜好量化。**

人类无法参与模型训练中的数亿次梯度下降，所以我们需要训练一个“代理人”来代表人类打分。

1. **收集偏好数据**：给模型一个 Prompt，让它生成多个不同的输出（如 A, B, C, D）。
    
2. **人类排序**：让人类对这些输出进行排序（例如 $A > B > C$）。
    
3. **训练模型**：训练一个较小的模型（RM），其目标函数是最大化正确排序的概率。
    
    - **输入**：一个 (Prompt, Response) 对。
        
    - **输出**：一个标量分数（Scalar Score）。分数越高，代表人类越喜欢这个回答。
        

---

## 3. 第三阶段：强化学习优化 (RL Optimization)

**目标：利用奖励模型，通过自博弈不断迭代。**

这是最核心的步骤，通常使用 **PPO (Proximal Policy Optimization)** 算法。

- **策略迭代**：模型针对一个问题生成答案，奖励模型（RM）给出一个分数。
    
- **梯度更新**：如果分数高，模型就会增加生成此类答案的概率；如果分数低，则降低概率。
    
- **引入约束（KL 散度）**：为了防止模型在追求高分的过程中变得“油腔滑调”或产生灾难性遗忘，我们会引入一个 KL 散度项，限制新模型不能偏离原始 SFT 模型太远。
    

$$Loss = E_{x, y \sim \pi_{\theta}} [RM(x, y)] - \beta KL(\pi_{\theta} || \pi_{SFT})$$

---

## 为什么 RLHF 是不可或缺的？

可以从以下三个维度理解其必要性：

1. **对齐 (Alignment)**：将模型的输出与人类的价值观、安全准则对齐。
    
2. **处理开放性问题**：很多问题没有唯一的“正确答案”（比如写诗、写代码风格），SFT 难以覆盖所有情况，而 RL 可以让模型在探索中找到最优解。
    
3. **解决幻觉 (Hallucination)**：通过奖励机制惩罚那些虽然概率高但事实错误的回答。
    

---

## 新趋势：DPO 与自对齐

虽然 RLHF 是 GPT-4 的功臣，但它极其复杂（需要维护四个模型：Actor, Critic, Ref, RM）。现在的趋势是：

- **[[DPO]] (Direct Preference Optimization)**：直接在偏好数据上微调，不需要显式训练奖励模型，也不需要复杂的 PPO 采样。它将 RL 问题转化为了一个简单的二元分类损失函数，在 2025-2026 年已成为主流。
    
- **RLAIF**：用 AI 替代人类进行反馈，实现自我进化。
    
